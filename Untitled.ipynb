{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c88fb81-98a6-4af3-9a9c-30c341efc141",
   "metadata": {},
   "source": [
    "# Used Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "3a2da1e5-b4a7-4381-923a-d1f64ae91989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Union, Tuple, Optional\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd035b-529d-4680-aaf8-c4b20f3b4ebe",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "398ebc83-11f4-40cb-9f7e-4f90f74c17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categoerical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, deriative=False) -> Union[float, Tuple[np.ndarray, np.ndarray]]:\n",
    "#     # Clip to prevent NaN's and Inf's to prevent log(0) or division by zero:\n",
    "#     y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "#     loss = -np.sum(y_true * np.log(y_pred), axis=-1)\n",
    "\n",
    "#     if deriative:\n",
    "#         grad = y_pred - y_true\n",
    "#         return loss, grad\n",
    "#     else:\n",
    "#         return loss\n",
    "    \n",
    "# def to_one_hot(y:np.ndarray, num_classes: int):\n",
    "#     one_hot_encoding = np.zeros((y.shape[0], num_classes))\n",
    "#     one_hot[np.arange(y.shape[0], y)] = 1\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "\n",
    "# def softmax_loss(x: np.ndarray, y_true: np.ndarray) -> float:\n",
    "#     y_pred = softmax(x)\n",
    "#     loss = categorical_cross_entropy(y_true, y_pred)\n",
    "#     return loss                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af8526-6922-4bd6-9d23-d6d2a6814fea",
   "metadata": {},
   "source": [
    "# Architecture of a single Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "29746e0c-6652-4c2d-b738-0c9116bb8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, hidden_units, activation: Optional[str]=None, random_seed=None):\n",
    "        \"\"\"\n",
    "        Connected layer for a neural network.\n",
    "\n",
    "        :param num_neurons: The number of neurons in the layer\n",
    "        :param activation: The activation function to use (if any)\n",
    "        \"\"\"\n",
    "        if activation not in [None, 'relu', 'softmax']:\n",
    "            raise KeyError('wrong activation function')\n",
    "            \n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.random_seed = random_seed if random_seed is not None else np.random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    def _init_params(self, input_size: int, hidden_units: int):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases for the layer.\n",
    "\n",
    "        :param input_size: The number of inputs to the layer\n",
    "        :param num_neurons: The number of neurons in the layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = np.random.RandomState(self.random_seed).randn(input_size, hidden_units) * np.sqrt(2. / input_size)\n",
    "        self.biases = np.zeros((1, hidden_units)) \n",
    "    \n",
    "    def _apply_activation(self, weighted_inputs: np.ndarray):\n",
    "        \"\"\"\n",
    "        Apply the activation function if any.\n",
    "        \"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(weighted_inputs)\n",
    "        elif self.activation == 'softmax':\n",
    "            return self.softmax(weighted_inputs)\n",
    "        else:\n",
    "            return weighted_inputs  # just return input\n",
    "        \n",
    "    def _apply_activation_backward(self, weighted_inputs: np.ndarray, grad: np.ndarray):\n",
    "        \"\"\"\n",
    "        Apply the activation function if any.\n",
    "        \"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu_backward(weighted_inputs, grad)\n",
    "        elif self.activation == 'softmax':\n",
    "            return self.softmax_backward(weighted_inputs, grad)\n",
    "        else:\n",
    "            return grad  # just return grad\n",
    "    \n",
    "    def forward_pass(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        self.inputs = inputs\n",
    "        if self.weights is None:\n",
    "            self._init_params(inputs.shape[-1], self.hidden_units)\n",
    "        \n",
    "        self.weighted_inputs = inputs @ self.weights + self.biases\n",
    "        return self._apply_activation(self.weighted_inputs)\n",
    "    \n",
    "    def backward_pass(self, next_layer_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the loss function with respect to the input of the layer and parameters.\n",
    "\n",
    "        :param next_layer_grad: The gradient of the loss function with respect to the output of the next layer.\n",
    "        :return: The gradient of the loss function with respect to the input of the layer (dL/dx), weights and biases.\n",
    "        \"\"\"\n",
    "        da = self._apply_activation_backward(self.weighted_inputs, next_layer_grad)\n",
    "        db = np.sum(da, axis=0)\n",
    "        dw = self.inputs.T @ da\n",
    "        dx = da @ self.weights.T\n",
    "        return dx, dw, db\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray, derivative=False) -> np.ndarray:\n",
    "        x = x - np.max(x)\n",
    "        s = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "        return s\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax_backward(x: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        x = x - np.max(x)\n",
    "        s = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "        \n",
    "        diag = np.stack([\n",
    "            np.diag(s[i])\n",
    "            for i in range(len(s))\n",
    "        ], 0)\n",
    "        softmax_grad = diag - np.einsum('bi,bj->bij', s, s)\n",
    "        # grad: batch_size x class_num\n",
    "        # softmax_grad: batch_size x class_num x class_num\n",
    "        return np.einsum('bc,bcd->bd', grad, softmax_grad)\n",
    "                \n",
    "    @staticmethod           \n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod           \n",
    "    def relu_backward(x, grad):\n",
    "        relu_grad = (x >= 0).astype(x.dtype)\n",
    "        return relu_grad * grad\n",
    "\n",
    "    # @property\n",
    "    # def params(self):\n",
    "    #     return [self.weights, self.biases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22949871-c777-48f1-8f41-94ebb02d5068",
   "metadata": {},
   "source": [
    "# Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "379b6fc1-e6d3-404d-a328-12d4a1054856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.velocity: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "\n",
    "    def update(self, layers, grads):\n",
    "        if not self.velocity:\n",
    "            self.velocity = [(np.zeros_like(layer.weights), np.zeros_like(layer.biases)) for layer in layers]\n",
    "\n",
    "        for (v_w, v_b), layer, (dw, db) in zip(self.velocity, layers, grads):\n",
    "            v_w *= self.momentum\n",
    "            v_w += self.learning_rate * dw\n",
    "            layer.weights -= v_w\n",
    "            \n",
    "            v_b *= self.momentum\n",
    "            v_b += self.learning_rate * db\n",
    "            layer.biases -= v_b\n",
    "            \n",
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-7):\n",
    "        super().__init__(learning_rate)\n",
    "        self.epsilon = epsilon\n",
    "        self.accumulated_grads: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    \n",
    "    def update(self, layers: List[Layer], grads: List[Tuple[np.ndarray, np.ndarray]]):\n",
    "        if not self.accumulated_grads:\n",
    "            self.accumulated_grads = [(np.zeros_like(layer.weights), np.zeros_like(layer.biases)) for layer in layers]\n",
    "\n",
    "        for (h_w, h_b), layer, (dw, db) in zip(self.accumulated_grads, layers, grads):\n",
    "            h_w += dw * dw\n",
    "            layer.weights -= self.learning_rate * dw / (np.sqrt(h_w) + self.epsilon)\n",
    "            \n",
    "            h_b += db * db\n",
    "            layer.biases -= self.learning_rate * db / (np.sqrt(h_b) + self.epsilon)\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam optimizer implementation.\n",
    "    https://optimization.cbe.cornell.edu/index.php?title=Adam\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-7):\n",
    "        \"\"\"\n",
    "        Initialize Adam optimizer.\n",
    "        \n",
    "        :param learning_rate: learning rate\n",
    "        :param beta1: The exponential decay rate for the first moment estimates\n",
    "        :param beta2: The exponential decay rate for the second-moment estimates\n",
    "        :param epsilon: small value to prevent division by zero\n",
    "        \"\"\"\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        self.v: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, layers: List[Layer], grads: List[Tuple[np.ndarray, np.ndarray]]) -> None:\n",
    "        \"\"\"\n",
    "        Perform the Adam update on parameters.\n",
    "\n",
    "        :param layers: list of layers with parameters to update\n",
    "        :param grads: list of gradients for each layer's parameters\n",
    "        \"\"\"\n",
    "        if not self.m:\n",
    "            self.m = [[np.zeros_like(layer.weights), np.zeros_like(layer.biases)] for layer in layers]\n",
    "            self.v = [[np.zeros_like(layer.weights), np.zeros_like(layer.biases)] for layer in layers]\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        for (m, v), layer, (dw, db) in zip(zip(self.m, self.v), layers, grads):\n",
    "            m[0] *= self.beta1\n",
    "            m[0] += (1.0 - self.beta1) * dw\n",
    "            bias_corrected_first_moment = m[0] / (1.0 - self.beta1**self.t)\n",
    "            v[0] *= self.beta2\n",
    "            v[0] += (1.0 - self.beta2) * dw**2\n",
    "            bias_corrected_second_moment = v[0] / (1.0 - self.beta2**self.t)\n",
    "            \n",
    "            layer.weights -= self.learning_rate * bias_corrected_first_moment / (np.sqrt(bias_corrected_second_moment) + self.epsilon)\n",
    "\n",
    "            m[1] *= self.beta1\n",
    "            m[1] += (1.0 - self.beta1) * db\n",
    "            bias_corrected_first_moment = m[1] / (1.0 - self.beta1**self.t)\n",
    "            v[1] *= self.beta2\n",
    "            v[1] += (1.0 - self.beta2) * db**2\n",
    "            bias_corrected_second_moment = v[1] / (1.0 - self.beta2**self.t)\n",
    "            \n",
    "            layer.biases -= self.learning_rate * bias_corrected_first_moment / (np.sqrt(bias_corrected_second_moment) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd5894-6bbb-49fb-8dd2-e0df74242a55",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "9bba68bf-eb90-4460-84b1-c1bef7f744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_epochs=100, batch_size=32, optimizer=SGD(learning_rate=0.01),\n",
    "                 activation='relu', hidden_sizes=[128], num_classes=4, verbose=False, random_seed=42):\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epochs = num_epochs\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.layers = [Layer(size, activation, random_seed=rng.randint(0, 2**32 - 1)) for size in hidden_sizes]\n",
    "        self.layers.append(Layer(num_classes, 'softmax', random_seed=rng.randint(0, 2**32 - 1)))\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    @staticmethod\n",
    "    def to_one_hot(y: np.ndarray, num_classes: int):\n",
    "        one_hot = np.zeros((y.shape[0], num_classes))\n",
    "        one_hot[range(y.shape[0]), y] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def categorical_cross_entropy(self, y_pred: np.ndarray, y_true: np.ndarray, derivative=False) -> Union[float, Tuple[np.ndarray, np.ndarray]]:\n",
    "        y_true = self.to_one_hot(y_true, self.num_classes)\n",
    "        \n",
    "        # Clip to prevent NaN's and Inf's to prevent log(0) or division by zero:\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if derivative:\n",
    "            return -y_true / y_pred\n",
    "            # return 2 * (y_pred - y_true)\n",
    "        else:\n",
    "            return -np.sum(y_true * np.log(y_pred), axis=-1)\n",
    "            # return ((y_true - y_pred) ** 2).sum(-1)\n",
    "    \n",
    "    def _create_mini_batches(self, X, y, batch_size, shuffle: bool, drop_last: bool):\n",
    "        \"\"\"\n",
    "        Creates mini-batches from the input data.\n",
    "\n",
    "        :param X: input features\n",
    "        :param y: targets\n",
    "        :param batch_size: size of the mini-batches\n",
    "        :yields: mini-batches\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            X, y = sklearn.utils.shuffle(X, y)\n",
    "        if drop_last:\n",
    "            n_minibatches = X.shape[0] // batch_size\n",
    "        else:\n",
    "            n_minibatches = (X.shape[0] + batch_size - 1) // batch_size\n",
    "        for i in range(n_minibatches):\n",
    "            yield X[i * batch_size : (i + 1) * batch_size], y[i * batch_size : (i + 1) * batch_size]\n",
    "    \n",
    "    def _forward(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Performs a forward pass throught the neural network.\n",
    "\n",
    "        :param X: input data\n",
    "        :return: the output of the last layer of the neural network, necessary to calculate backprop\n",
    "        \"\"\"\n",
    "        if not self.layers:\n",
    "            raise ValueError(\"No layers in the neural network.\")\n",
    "\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward_pass(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _backpropagation(self, y_pred: np.ndarray, y_true: np.ndarray):\n",
    "        \"\"\"\n",
    "        Executes the backpropagation algorithm.\n",
    "\n",
    "        The goal of backpropagation is to compute the gradient of the loss function with respect to the weights of the network,\n",
    "        which is done by propagating the gradient backwards through the network. The gradients are then used to update the weights and biases to minimize the loss\n",
    "        :param y_pred: network's output from the forward pass algorithm\n",
    "        :param y_true: true labels \n",
    "        \"\"\"\n",
    "        # Calculate the initial gradient as the derivative of the loss function\n",
    "        grads = []\n",
    "        dx = self.categorical_cross_entropy(y_pred, y_true, derivative=True)\n",
    "\n",
    "        for layer in self.layers[::-1]:\n",
    "            # Calculate the gradient at the current layer\n",
    "            dx, dw, db = layer.backward_pass(dx)\n",
    "            grads.append((dw, db))\n",
    "\n",
    "        grads = grads[::-1]\n",
    "        self.optimizer.update(self.layers, grads) # Update the weights and biases for all layers\n",
    "\n",
    "    def _run_single_epoch(self, X, y, optimize: bool):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for X_batch, y_batch in self._create_mini_batches(X, y, self.batch_size, shuffle=optimize, drop_last=optimize):\n",
    "            y_pred = self._forward(X_batch)\n",
    "\n",
    "            loss = self.categorical_cross_entropy(y_pred, y_batch)\n",
    "            total_loss += loss.sum()\n",
    "\n",
    "            correct = self.count_correct_predictions(y_batch, y_pred)\n",
    "            correct_predictions += correct\n",
    "\n",
    "            total_samples += len(X_batch)\n",
    "\n",
    "            if optimize:\n",
    "                self._backpropagation(y_pred, y_batch)\n",
    "\n",
    "        average_loss = total_loss / total_samples\n",
    "        average_accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        return average_loss, average_accuracy\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "        \n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            train_loss, train_accuracy = self._run_single_epoch(X_train, y_train, optimize=True)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_accuracy'].append(train_accuracy)\n",
    "            \n",
    "            val_loss, val_accuracy = self._run_single_epoch(X_val, y_val, optimize=False)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_accuracy)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'Epoch {epoch + 1}: '\n",
    "                      f'train_loss={train_loss} train_accuracy={train_accuracy} '\n",
    "                      f'val_loss={val_loss} val_accuracy={val_accuracy}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_correct_predictions(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Counts the number of correct predictions.\n",
    "\n",
    "        :param y_true: true labels\n",
    "        :param y_pred: predicted labels\n",
    "        :return: number of correct predictions\n",
    "        \"\"\"\n",
    "        return np.sum(y_true == y_pred.argmax(axis=-1))\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Evaluates the network's performance on the provided data.\n",
    "\n",
    "        :param X: The input data\n",
    "        :return: class predictions\n",
    "        \"\"\"\n",
    "        X = self.scaler.transform(X)\n",
    "        y_pred = self.forward(X)\n",
    "        return y_pred.argmax(-1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "eb4035d0-5598-4d55-8ae8-7fdb39cf7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    Loads data and splits them to X and Y\n",
    "    '''\n",
    "    data = load_wine()\n",
    "    return data['data'], data['target']\n",
    "    \n",
    "def plot_metrics(train_accuracy: list, train_loss: list, epochs: list):\n",
    "    \"\"\"\n",
    "    Function to plot training accuracy and loss.\n",
    "\n",
    "    :param train_accuracy: List of training accuracy for each epoch\n",
    "    :param train_loss: List of training loss for each epoch\n",
    "    :param epochs: List of epochs\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x=epochs, y=train_accuracy, marker='o', color='b')\n",
    "    plt.title('Training Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=epochs, y=train_loss, marker='o', color='r')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8371157-c924-473f-9018-71f9708c1598",
   "metadata": {},
   "source": [
    "# Data split -- test, train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3ead64ff-d24e-43fc-b322-4215a77ef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "56eb67cf-07b1-449d-a3bc-d16032890dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, Y, model_args, n_splits=5):\n",
    "    model_args = model_args.copy()\n",
    "    optim = model_args.pop('optimizer').copy()\n",
    "    optim_name = optim.pop('name')\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=model_args.get('random_seed', 0))\n",
    "    acc = []\n",
    "    for i, (train_indices, val_indices) in enumerate(skf.split(X, Y)):\n",
    "        if optim_name == 'SGD':\n",
    "            opt = SGD(**optim)\n",
    "        elif optim_name == 'Adam':\n",
    "            opt = Adam(**optim)\n",
    "        elif optim_name == 'AdaGrad':\n",
    "            opt = AdaGrad(**optim)\n",
    "        else:\n",
    "            raise ValueError('wrong optimizer')\n",
    "            \n",
    "        model = NeuralNetwork(optimizer=opt, **model_args)\n",
    "        model.fit(X[train_indices], Y[train_indices], X[val_indices], Y[val_indices])\n",
    "        acc.append(model.history['val_accuracy'][-1])\n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1e89086b-6a19-40a2-86fb-26fb079aef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X, Y, iterations, reps, pool=__builtins__):\n",
    "    '''\n",
    "    performs a random search to find best hyperparameters for the model.\n",
    "    `iterations` different hyperparameters are randomized, and evaluated `reps` times (with different random seeds)\n",
    "    returns best hyperparameters and accuracy\n",
    "    '''\n",
    "    args = [\n",
    "        dict(hidden_sizes=[round(2 ** np.random.randint(0, 8)) for _ in range(np.random.randint(0, 4))],\n",
    "             batch_size=np.random.randint(1, 32),\n",
    "             num_epochs=np.random.randint(1, 100),\n",
    "             optimizer=np.random.choice([\n",
    "                 {\n",
    "                     'name': 'SGD',\n",
    "                     'learning_rate': 10 ** np.random.normal(-3, 0),\n",
    "                     'momentum': np.random.uniform(0, 0.99),\n",
    "                 },\n",
    "                 {\n",
    "                     'name': 'Adam',\n",
    "                     'learning_rate': 10 ** np.random.normal(-3, 0),\n",
    "                     'beta1': np.random.uniform(0, 0.99),\n",
    "                     'beta2': np.random.uniform(0, 0.99),\n",
    "                 },\n",
    "                 {\n",
    "                     'name': 'AdaGrad',\n",
    "                     'learning_rate': 10 ** np.random.normal(-3, 0),\n",
    "                 },\n",
    "             ])\n",
    "        ) for i in range(iterations)\n",
    "    ]\n",
    "    accs = np.array([\n",
    "        list(pool.map(partial(evaluate_model, X, Y), [a | {'random_seed': j} for a in args]))\n",
    "        for j in range(reps)\n",
    "    ])\n",
    "    accs = accs.mean(0)\n",
    "    i = accs.argmax()\n",
    "    return args[i], accs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "4ecf5400-09fa-44f1-839b-608fe0df03cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 s, sys: 7.95 s, total: 38.9 s\n",
      "Wall time: 28.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'hidden_sizes': [],\n",
       "  'batch_size': 6,\n",
       "  'num_epochs': 52,\n",
       "  'optimizer': {'name': 'SGD',\n",
       "   'learning_rate': 0.001,\n",
       "   'momentum': 0.9245579569449347}},\n",
       " 0.9830952380952382)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(4) as pool:\n",
    "    best_args, best_accuracy = random_search(X, Y, 16, 2)\n",
    "best_args, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4e8b6a59-5bd0-43b0-8011-8e21fb1bd31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02207353730149884\n",
      "0.9776190476190477\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "history = []\n",
    "for i, (train_indices, val_indices) in enumerate(skf.split(X, Y)):\n",
    "    model = NeuralNetwork(optimizer=AdaGrad(learning_rate=0.1), num_epochs=100, hidden_sizes=[], batch_size=32)\n",
    "    model.fit(X[train_indices], Y[train_indices], X[val_indices], Y[val_indices])\n",
    "    history.append(model.history)\n",
    "    \n",
    "history = {key: np.mean([h[key] for h in history], 0) for key in history[0].keys()}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(pd.DataFrame(history)[['train_accuracy', 'val_accuracy']])\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(pd.DataFrame(history)[['train_loss', 'val_loss']])\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "print(history['train_loss'][-1])\n",
    "print(history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ca042-01a7-432f-883a-69c4f4dd48dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cbc35-9f84-4958-8503-6ab11b5aebe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
