{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c88fb81-98a6-4af3-9a9c-30c341efc141",
   "metadata": {},
   "source": [
    "# Used Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2da1e5-b4a7-4381-923a-d1f64ae91989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Union, Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd035b-529d-4680-aaf8-c4b20f3b4ebe",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398ebc83-11f4-40cb-9f7e-4f90f74c17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoerical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, deriative=False) -> Union[float, Tuple[np.ndarray, np.ndarray]]:\n",
    "    # Clip to prevent NaN's and Inf's to prevent log(0) or division by zero:\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    loss = -np.sum(y_true * np.log(y_pred), axis=-1)\n",
    "\n",
    "    if deriative:\n",
    "        grad = y_pred - y_true\n",
    "        return loss, grad\n",
    "    else:\n",
    "        return loss\n",
    "    \n",
    "def to_one_hot(y:np.ndarray, num_classes: int):\n",
    "    one_hot_encoding = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0], y)] = 1\n",
    "    return one_hot\n",
    "\n",
    "                      \n",
    "def softmax(x: np.ndarray, derivative=False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "    softmax_output = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    if derivative:\n",
    "        s = softmax_output.reshape(-1, 1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "    else:\n",
    "        return softmax_output\n",
    "        \n",
    "def softmax_stable(x: np.ndarray):\n",
    "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "                \n",
    "                \n",
    "def relu(x, deriative=False):\n",
    "    if deriative:\n",
    "        grad = np.zeros_like(x)\n",
    "        grad = np.where(x >= 0, 1, grad)\n",
    "        return grad\n",
    "    else:\n",
    "        return np.maximum(0, x)       \n",
    "\n",
    "def softmax_loss(x: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    y_pred = softmax(x)\n",
    "    loss = categorical_cross_entropy(y_true, y_pred)\n",
    "    return loss                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af8526-6922-4bd6-9d23-d6d2a6814fea",
   "metadata": {},
   "source": [
    "# Architecture of a single Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29746e0c-6652-4c2d-b738-0c9116bb8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, hidden_units, activation_fun: str=None):\n",
    "         \"\"\"\n",
    "        Connected layer for a neural network.\n",
    "\n",
    "        :param num_neurons: The number of neurons in the layer\n",
    "        :param activation: The activation function to use (if any)\n",
    "        \"\"\"\n",
    "            \n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation_fun = activation_fun\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "    \n",
    "    def _init_params(self, input_size: int, hidden_units: int):\n",
    "         \"\"\"\n",
    "        Initialize the weights and biases for the layer.\n",
    "\n",
    "        :param input_size: The number of inputs to the layer\n",
    "        :param num_neurons: The number of neurons in the layer\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n",
    "        self.biases = np.zeros((1, num_neurons)) \n",
    "    \n",
    "    def _apply_activation(self, weighted_input: np.ndarray)\n",
    "        \"\"\"\n",
    "        Apply the activation function if any.\n",
    "        \"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(weighted_input, derivative)\n",
    "        elif self.activation == 'softmax':\n",
    "            return self.softmax(weighted_input, derivative)\n",
    "        else:\n",
    "            return weighted_input # just return input\n",
    "\n",
    "    \n",
    "    def forward_pass(self, inputs: np.ndarray) -> np.ndarray\n",
    "        self.inputs = inputs\n",
    "        if self.weights is None:\n",
    "            self._init_params(inputs.shape[-1], self.hidden_units)\n",
    "        \n",
    "        self.weighted_inputs = inputs @ self.weights + self.biases\n",
    "        if self.activation:\n",
    "            self.outputs = self._apply_activation(self.weighted_inputs)\n",
    "        else:\n",
    "            self.outputs= self.weighted_inputs\n",
    "        \n",
    "        return self.outputs\n",
    "    \n",
    "    @staticmathod\n",
    "    def softmax(x: np.ndarray, derivative=False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "        softmax_output = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        \n",
    "        if derivative:\n",
    "            s = softmax_output.reshape(-1, 1)\n",
    "            return np.diagflat(s) - np.dot(s, s.T)\n",
    "        else:\n",
    "            return softmax_output\n",
    "    \n",
    "    @staticmathod\n",
    "    def softmax_stable(x: np.ndarray):\n",
    "        return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "                \n",
    "    @staticmathod           \n",
    "    def relu(x, deriative=False):\n",
    "        if deriative:\n",
    "            grad = np.zeros_like(x)\n",
    "            grad = np.where(x >= 0, 1, grad)\n",
    "            return grad\n",
    "        else:\n",
    "            return np.maximum(0, x)       \n",
    "\n",
    "\n",
    "    def softmax_loss(x: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        y_pred = softmax(x)\n",
    "        loss = categorical_cross_entropy(y_true, y_pred)\n",
    "        return loss\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.weights, self.biases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22949871-c777-48f1-8f41-94ebb02d5068",
   "metadata": {},
   "source": [
    "# Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379b6fc1-e6d3-404d-a328-12d4a1054856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.velocity: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "\n",
    "    def update(self, layers, grads):\n",
    "        if not self.velocity:\n",
    "            self.velocity = [(np.zeros_like(layer.weights), np.zeros_like(layer.biases)) for layer in layers]\n",
    "\n",
    "        for (v_w, v_b), layer, (dw, db) in zip(self.velocity, layers, grads):\n",
    "            v_w = self.momentum * v_w + self.learning_rate * dw\n",
    "            layer.weights -= v_w\n",
    "            \n",
    "            v_b = self.momentum * v_b + self.learning_rate * db\n",
    "            layer.biases -= v_b\n",
    "            \n",
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-7):\n",
    "        super().__init__(learning_rate)\n",
    "        self.epsilon = epsilon\n",
    "        self.accumulated_grads: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    \n",
    "    def update(self, layers: List[Layer], grads: List[Tuple[np.ndarray, np.ndarray]]):\n",
    "        if not self.accumulated_grads:\n",
    "            self.accumulated_grads = [(np.zeros_like(layer.weights), np.zeros_like(layer.biases)) for layer in layers]\n",
    "\n",
    "        for (h_w, h_b), layer, (dw, db) in zip(self.h, layers, accumulated_grads):\n",
    "            accumulated_grads_w += dw * dw\n",
    "            layer.weights -= self.learning_rate * dw / (np.sqrt(accumulated_grads_w) + self.epsilon)\n",
    "            \n",
    "            accumulated_grads_b += db * db\n",
    "            layer.biases -= self.learning_rate * db / (np.sqrt(accumulated_grads_b) + self.epsilon)\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam optimizer implementation.\n",
    "    https://optimization.cbe.cornell.edu/index.php?title=Adam\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-7):\n",
    "        \"\"\"\n",
    "        Initialize Adam optimizer.\n",
    "        \n",
    "        :param learning_rate: learning rate\n",
    "        :param beta1: The exponential decay rate for the first moment estimates\n",
    "        :param beta2: The exponential decay rate for the second-moment estimates\n",
    "        :param epsilon: small value to prevent division by zero\n",
    "        \"\"\"\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        self.v: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, layers: List[Layer], grads: List[Tuple[np.ndarray, np.ndarray]]) -> None:\n",
    "        \"\"\"\n",
    "        Perform the Adam update on parameters.\n",
    "\n",
    "        :param layers: list of layers with parameters to update\n",
    "        :param grads: list of gradients for each layer's parameters\n",
    "        \"\"\"\n",
    "        if not self.m:\n",
    "            self.m = [(np.zeros_like(layer.weights), np.zeros_like(layer.biases)) for layer in layers]\n",
    "            self.v = [(np.zeros_like(layer.weights), np.zeros_like(layer.biases)) for layer in layers]\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        for (m, v), layer, (dw, db) in zip(zip(self.m, self.v), layers, grads):\n",
    "            m[0] = self.beta1 * m[0] + (1.0 - self.beta1) * dw\n",
    "            bias_corrected_first_moment = m[0] / (1.0 - self.beta1**self.t)\n",
    "            v[0] = self.beta2 * v[0] + (1.0 - self.beta2) * dw**2\n",
    "            bias_corrected_second_moment = v[0] / (1.0 - self.beta2**self.t)\n",
    "            \n",
    "            updated_weights = layer.weights - self.learning_rate * bias_corrected_first_moment / (np.sqrt(bias_corrected_second_moment) + self.epsilon)\n",
    "            layer.weights = updated_weights\n",
    "\n",
    "            m[1] = self.beta1 * m[1] + (1.0 - self.beta1) * db\n",
    "            bias_corrected_first_moment = m[1] / (1.0 - self.beta1**self.t)\n",
    "            v[1] = self.beta2 * v[1] + (1.0 - self.beta2) * db**2\n",
    "            bias_corrected_second_moment = v[1] / (1.0 - self.beta2**self.t)\n",
    "            \n",
    "            updated_biases = layer.biases - self.learning_rate * bias_corrected_first_moment / (np.sqrt(bias_corrected_second_moment) + self.epsilon)\n",
    "            layer.biases = updated_biases\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd5894-6bbb-49fb-8dd2-e0df74242a55",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bba68bf-eb90-4460-84b1-c1bef7f744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.01, num_epochs=100, verbose=False):\n",
    "        self.layers = []\n",
    "        self.optimizer = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def add_layer(self, layer: Layer)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set_optimizer(self, optimizer)\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward(self, X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Performs a forward pass throught the neural network.\n",
    "    \n",
    "    :param X: input data\n",
    "    :return: the output of the last layer of the neural network, necessary to calculate backprop\n",
    "    \"\"\"\n",
    "    if not self.layers:\n",
    "        raise ValueError(\"No layers in the neural network.\")\n",
    "    \n",
    "    for layer in self.layers:\n",
    "        X = layer.forward(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4035d0-5598-4d55-8ae8-7fdb39cf7bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
